# Data Retention in Kafka

* Unlike a _database_ - _Kafka_ is usually used to pipe enormous amounts of Data hence the Data is not stored indefinitely.
* Since Data is eventually Deleted, given a restart of your Stream Processing Application - it might not find all of the previously seen data.
* There are generally __two strategies__ to implement __Data Retention in Kafka__ :
  * __Deletion__ and 
  * __Log Compaction__
* __Data Retention Strategies__ are enabled by setting __ğ—¹ğ—¼ğ—´.ğ—°ğ—¹ğ—²ğ—®ğ—»ğ˜‚ğ—½.ğ—½ğ—¼ğ—¹ğ—¶ğ—°ğ˜†__ to _ğ—±ğ—²ğ—¹ğ—²ğ˜ğ—²_ or _ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—°ğ˜_.
* This is set on a Broker level but can later be configured per Topic.

# Deletion
* Each Partition in a Topic is made of Log Segments.
* Log Segments are closed on a certain condition. It can be on a segment reaching a certain size or it being open for a certain time.
* Closed Log Segments are marked for deletion if the difference between current time and when the segment was closed is more than one of: __ğ—¹ğ—¼ğ—´.ğ—¿ğ—²ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—».ğ—ºğ˜€__, __ğ—¹ğ—¼ğ—´.ğ—¿ğ—²ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—».ğ—ºğ—¶ğ—»ğ˜‚ğ˜ğ—²ğ˜€__, __ğ—¹ğ—¼ğ—´.ğ—¿ğ—²ğ˜ğ—²ğ—»ğ˜ğ—¶ğ—¼ğ—».ğ—µğ—¼ğ˜‚ğ—¿ğ˜€__. If all of them are set, a more granular option will be used.
* After being marked for deletion the segments will be cleaned up by background processes.

# Log Compaction
* This Strategy only works on Keyed Partitions.
* Topics are compacted by retaining only the latest written record per key.
* Compaction is performed by the background process consisting of a pool of threads called Log Cleaner.